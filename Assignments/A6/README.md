### A6.1 (5分) 分析卷积神经网络中用1×1的卷积核的作用，试分析大的卷积核和小的卷积核各自的优缺点。

#### 1×1 卷积核的作用
1. **维度改变和减少计算量：** 1×1的卷积核可以在不改变特征图尺寸的前提下，改变其深度。这意味着它可以用来减少网络中的参数数量，从而减少计算量和计算开销。
2. **增加非线性和网络深度：** 利用$1×1$卷积后的非线性激活函数（如tanh），我们可以在保持特征图尺寸不变的前提下，大幅增加网络层之间的非线性，从而学习到更加复杂的特征。
3. **实现信息的跨通道交互和整合：** 考虑到卷积运算的输入输出都是3个维度（宽、高、多通道），所以$1×1$卷积实际上就是对每个像素点在不同的通道上进行线性组合，从而整合不同通道的信息。这种跨通道的信息交互和整合，可以帮助网络学习到更加丰富的特征。

#### 大卷积核的优缺点
**优点：**
1. **空间特征提取：** 较大的卷积核（如3×3, 5×5）能够捕捉到更大范围的空间特征。
2. **感受野增加：** 大卷积核有更大的感受野，可以获取更多的上下文信息。

**缺点：**
1. **参数多，计算量大：** 大卷积核会增加网络的参数数量和计算量。
2. **过拟合风险：** 更多的参数可能导致网络更容易过拟合。

#### 小卷积核的优缺点
**优点：**
1. **减少参数：** 小卷积核，尤其是1×1，大大减少了模型的参数量。
2. **增加非线性：** 小卷积核可以增加网络的深度，从而增加模型的非线性。

**缺点：**
1. **感受野小：** 小卷积核的感受野较小，可能无法有效捕捉到更大范围的特征。
2. **空间特征提取能力有限：** 相比于大卷积核，小卷积核在空间特征的提取能力上有所不足。

### A6.2 (5分) 计算函数$𝑦 = max(𝑥1,⋯,𝑥𝐷)$和函数$𝑦 = argmax(𝑥1,⋯,𝑥𝐷)$的梯度。

1. 函数 $y = \max(x_1, \ldots, x_D)$：
   - 这个函数取输入向量 $(x_1, \ldots, x_D)$ 中的最大值。
   - 梯度是一个向量，其在最大值对应的位置是1，其他位置是0。
   - 举例来说，如果输入向量是 $(1, 2, 3, 2)$，那么最大值是3，梯度向量是 $(0, 0, 1, 0)$。

2. 函数 $ y = \text{argmax}(x_1, \ldots, x_D) $：
   - `argmax` 返回输入向量中最大值的索引。
   - 由于 `argmax` 输出的是一个索引（即离散的整数值），而不是连续的实数，所以它在数学上是不可微的。当输入的微小变化导致 argmax 输出的索引发生突变时，这种突变性质使得无法定义梯度。
        - 例如，如果输入向量稍微变化，使得原本第二大的值成为最大值，argmax 输出的索引会突然改变。这种突变意味着在大多数点上，函数要么没有斜率（因为输出不变），要么斜率是无限的（因为输出突然跳变）。
   - 当需要处理涉及 `argmax` 梯度计算的情形时，可以考虑使用 `softmax` 函数作为 `argmax` 的平滑近似。`softmax` 函数提供了一种方法来输出输入向量的概率分布，这些概率表示每个元素成为最大值的可能性​，其连续且可微分。

### A6.3 (5分) 推导LSTM网络中参数的梯度，并分析其避免梯度消失的效果。

### A6.4 (5分) 当将自注意力模型作为神经网络的一层使用时，分析它和卷积层以及循环层在建模长距离依赖关系的效率和计算复杂度方面的差异。
